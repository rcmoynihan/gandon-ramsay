{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MIS 285N Cognitive Computing<br>\n",
    "Final Project<br>\n",
    "Jerry Che - Jose Guerrero - Riley Moynihan - Noah Placke - Sarah Teng - Palmer Wenzel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions Generation Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probabilistic technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read data from CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>steps</th>\n",
       "      <th>crabmeat</th>\n",
       "      <th>creamcheese</th>\n",
       "      <th>greenonions</th>\n",
       "      <th>garlicsalt</th>\n",
       "      <th>refrigeratedcrescentdinnerrolls</th>\n",
       "      <th>eggyolk</th>\n",
       "      <th>water</th>\n",
       "      <th>sesameseeds</th>\n",
       "      <th>...</th>\n",
       "      <th>tex-mexseasoning</th>\n",
       "      <th>lightnon-dairywhippedtopping</th>\n",
       "      <th>stelladoroanginetticookies</th>\n",
       "      <th>viennabread</th>\n",
       "      <th>beefroundrumproast</th>\n",
       "      <th>romaineleaf</th>\n",
       "      <th>nuocnam</th>\n",
       "      <th>thaiholybasil</th>\n",
       "      <th>driedblacktrumpetmushrooms</th>\n",
       "      <th>driedwoodearmushrooms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>crab filled crescent snacks</td>\n",
       "      <td>heat over to 375 degrees, spray large cookie s...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>curried bean salad</td>\n",
       "      <td>drain &amp; rinse beans, stir all ingredients toge...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>delicious steak with onion marinade</td>\n",
       "      <td>heat the oil in a heavy-based pan and cook the...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 7686 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  name  \\\n",
       "0          crab filled crescent snacks   \n",
       "1                   curried bean salad   \n",
       "2  delicious steak with onion marinade   \n",
       "\n",
       "                                               steps  crabmeat  creamcheese  \\\n",
       "0  heat over to 375 degrees, spray large cookie s...       1.0          1.0   \n",
       "1  drain & rinse beans, stir all ingredients toge...       0.0          0.0   \n",
       "2  heat the oil in a heavy-based pan and cook the...       0.0          0.0   \n",
       "\n",
       "   greenonions  garlicsalt  refrigeratedcrescentdinnerrolls  eggyolk  water  \\\n",
       "0          1.0         1.0                              1.0      1.0    1.0   \n",
       "1          0.0         0.0                              0.0      0.0    0.0   \n",
       "2          0.0         0.0                              0.0      0.0    0.0   \n",
       "\n",
       "   sesameseeds  ...  tex-mexseasoning  lightnon-dairywhippedtopping  \\\n",
       "0          1.0  ...               0.0                           0.0   \n",
       "1          0.0  ...               0.0                           0.0   \n",
       "2          0.0  ...               0.0                           0.0   \n",
       "\n",
       "   stelladoroanginetticookies  viennabread  beefroundrumproast  romaineleaf  \\\n",
       "0                         0.0          0.0                 0.0          0.0   \n",
       "1                         0.0          0.0                 0.0          0.0   \n",
       "2                         0.0          0.0                 0.0          0.0   \n",
       "\n",
       "   nuocnam  thaiholybasil  driedblacktrumpetmushrooms  driedwoodearmushrooms  \n",
       "0      0.0            0.0                         0.0                    0.0  \n",
       "1      0.0            0.0                         0.0                    0.0  \n",
       "2      0.0            0.0                         0.0                    0.0  \n",
       "\n",
       "[3 rows x 7686 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# pd.options.display.max_columns = 500\n",
    "\n",
    "\n",
    "df = pd.read_csv('../data/kaggle/processed/recipes_processed.csv')#.sample(frac=0.1, random_state=42)\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop unnecessary columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>steps</th>\n",
       "      <th>crabmeat</th>\n",
       "      <th>creamcheese</th>\n",
       "      <th>greenonions</th>\n",
       "      <th>garlicsalt</th>\n",
       "      <th>refrigeratedcrescentdinnerrolls</th>\n",
       "      <th>eggyolk</th>\n",
       "      <th>water</th>\n",
       "      <th>sesameseeds</th>\n",
       "      <th>sweetandsoursauce</th>\n",
       "      <th>...</th>\n",
       "      <th>tex-mexseasoning</th>\n",
       "      <th>lightnon-dairywhippedtopping</th>\n",
       "      <th>stelladoroanginetticookies</th>\n",
       "      <th>viennabread</th>\n",
       "      <th>beefroundrumproast</th>\n",
       "      <th>romaineleaf</th>\n",
       "      <th>nuocnam</th>\n",
       "      <th>thaiholybasil</th>\n",
       "      <th>driedblacktrumpetmushrooms</th>\n",
       "      <th>driedwoodearmushrooms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>heat over to 375 degrees, spray large cookie s...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>drain &amp; rinse beans, stir all ingredients toge...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>heat the oil in a heavy-based pan and cook the...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 7685 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               steps  crabmeat  creamcheese  \\\n",
       "0  heat over to 375 degrees, spray large cookie s...       1.0          1.0   \n",
       "1  drain & rinse beans, stir all ingredients toge...       0.0          0.0   \n",
       "2  heat the oil in a heavy-based pan and cook the...       0.0          0.0   \n",
       "\n",
       "   greenonions  garlicsalt  refrigeratedcrescentdinnerrolls  eggyolk  water  \\\n",
       "0          1.0         1.0                              1.0      1.0    1.0   \n",
       "1          0.0         0.0                              0.0      0.0    0.0   \n",
       "2          0.0         0.0                              0.0      0.0    0.0   \n",
       "\n",
       "   sesameseeds  sweetandsoursauce  ...  tex-mexseasoning  \\\n",
       "0          1.0                1.0  ...               0.0   \n",
       "1          0.0                0.0  ...               0.0   \n",
       "2          0.0                0.0  ...               0.0   \n",
       "\n",
       "   lightnon-dairywhippedtopping  stelladoroanginetticookies  viennabread  \\\n",
       "0                           0.0                         0.0          0.0   \n",
       "1                           0.0                         0.0          0.0   \n",
       "2                           0.0                         0.0          0.0   \n",
       "\n",
       "   beefroundrumproast  romaineleaf  nuocnam  thaiholybasil  \\\n",
       "0                 0.0          0.0      0.0            0.0   \n",
       "1                 0.0          0.0      0.0            0.0   \n",
       "2                 0.0          0.0      0.0            0.0   \n",
       "\n",
       "   driedblacktrumpetmushrooms  driedwoodearmushrooms  \n",
       "0                         0.0                    0.0  \n",
       "1                         0.0                    0.0  \n",
       "2                         0.0                    0.0  \n",
       "\n",
       "[3 rows x 7685 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop(['name'], axis=1)\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separate steps from ingredients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = df['steps']\n",
    "ingredients = df.drop(['steps'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create an ngram model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict, Counter\n",
    "import random\n",
    "\n",
    "\n",
    "class N_Gram:\n",
    "    \"\"\"\n",
    "    Create a token level ngram model\n",
    "    Uses the idea of the \"sliding window\" technique to predict the next word based on the n previous words\n",
    "    Makes predictions based on probability distributions of all ngrams found in the corpus\n",
    "    \n",
    "    Adapted from: https://nbviewer.jupyter.org/gist/yoavg/d76121dfde2618422139\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n=2, debug=False):\n",
    "        self.n = n\n",
    "        self.tokens = []\n",
    "        self.ngram_probas = {}\n",
    "        self.token_probas = {}\n",
    "        self.debug = debug\n",
    "    \n",
    "    \n",
    "    def fit(self, corpus, min_df=.1):\n",
    "        \"\"\"Train the model on a given corpus\"\"\"\n",
    "        \n",
    "        for document in corpus:\n",
    "            # Tokenize text into words\n",
    "            self.tokens = word_tokenize(document)\n",
    "            \n",
    "            self.remove_below_min_df(min_df)\n",
    "            \n",
    "            # Create n-grams from tokens\n",
    "            ngrams = zip(*[self.tokens[i:] for i in range(self.n + 1)])\n",
    "            \n",
    "            # Get count of next token after each ngram\n",
    "            ngram_counts = defaultdict(Counter)\n",
    "            for ngram in ngrams:\n",
    "                ngram_counts[ngram[:-1]][ngram[-1]] += 1\n",
    "\n",
    "            # Normalize counts\n",
    "            for ngram, counter in ngram_counts.items():\n",
    "                s = float(sum(counter.values()))\n",
    "                self.ngram_probas[ngram] = sorted([(c,cnt/s) for c,cnt in counter.items()], key=lambda x: x[1], reverse=True)\n",
    "                \n",
    "            self.calc_token_distribution()\n",
    "              \n",
    "                \n",
    "    def calc_token_distribution(self):\n",
    "        \"\"\"Get distribution of individual tokens, for instances where we may have a new ngram\"\"\"\n",
    "        # Reset token probabilities\n",
    "        self.token_probas = {}\n",
    "        \n",
    "        token_counts = defaultdict(int)\n",
    "        for token in self.tokens:\n",
    "            token_counts[token] += 1\n",
    "            \n",
    "        # Get total number of words\n",
    "        total_tokens = len(self.tokens)\n",
    "\n",
    "        # Compute probability of each token\n",
    "        for token, count in token_counts.items():\n",
    "            self.token_probas[token] = count / total_tokens\n",
    "            \n",
    "            \n",
    "    def remove_below_min_df(self, min_df):\n",
    "        \"\"\"Remove tokens that fall below the min_df threshold\"\"\"\n",
    "        \n",
    "        # Do initial calculation of token probabilities\n",
    "        self.calc_token_distribution()\n",
    "        \n",
    "        # Find tokens in the bottom min_df%\n",
    "        token_list = sorted([(token, proba) for token, proba in self.token_probas.items()], key=lambda x: x[1], reverse=True)\n",
    "        token_list = [token for token, proba in token_list]\n",
    "        bottom_tokens = token_list[int(len(token_list) * (1 - min_df)) : ]\n",
    "        \n",
    "        # Filter tokens in the bottom %\n",
    "        self.tokens = [token for token in self.tokens if token not in bottom_tokens]\n",
    "        \n",
    "        # Recalculate token distrubution\n",
    "        self.calc_token_distribution()\n",
    "            \n",
    "            \n",
    "    def generate_text(self, seed_tokens, num_words=200, randomness='weighted'):\n",
    "        \"\"\"Synthesize a body of text for a given number of words\"\"\"\n",
    "        \n",
    "        # Check to make sure the right number of seed tokens were given\n",
    "        if len(seed_tokens) != self.n:\n",
    "            raise Exception(f\"Number of seed tokens does not equal n (expected {self.n} seed tokens, but was given {len(seed_tokens)}\")\n",
    "            \n",
    "        # Lower seed tokens\n",
    "        text = [token.lower() for token in seed_tokens]\n",
    "        \n",
    "        # Generate text\n",
    "        keyerror_count = 0\n",
    "        while (len(text) < num_words or '.' not in text[-1]) and (len(text) < num_words * 1.5):\n",
    "            # Get the current n-gram\n",
    "            current_ngram = tuple(text[-self.n:])\n",
    "            \n",
    "            try:\n",
    "                # Sample the next token from this distribution for this n-gram evenly\n",
    "                if randomness == 'full':\n",
    "                    next_token = random.choice([t for t, p in self.ngram_probas[current_ngram]])\n",
    "\n",
    "                # Sample the next token from this distribution for this n-gram, using probability as weights\n",
    "                elif randomness == 'weighted':\n",
    "                    next_token = random.choices([t for t, p in self.ngram_probas[current_ngram]], weights=[p for t, p in self.ngram_probas[current_ngram]], k=1)[0]\n",
    "\n",
    "                # Use the max probability option for each ngram\n",
    "                elif randomness == 'none':\n",
    "                    next_token = self.ngram_probas[current_ngram][0][0]\n",
    "\n",
    "                else:\n",
    "                    raise Exception(\"Invalid option for randomness. Must be 'full', 'weighted', or 'none'.\")\n",
    "            \n",
    "            # Encountered ngram we haven't seen before, take random token from total corpus distribution (with weights)\n",
    "            except KeyError:\n",
    "                keyerror_count += 1\n",
    "                next_token = random.choices(list(self.token_probas.keys()), weights=list(self.token_probas.values()), k=1)[0]\n",
    "            \n",
    "            # Append to generated text\n",
    "            text.append(next_token)\n",
    "        \n",
    "        if self.debug:\n",
    "            print(keyerror_count)\n",
    "        \n",
    "        # Return as string\n",
    "        return ' '.join(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit the model on the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit the model\n",
    "model = N_Gram(n=3)\n",
    "model.fit(steps.values, min_df=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate a random paragraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to give the model some \"seed tokens\". Since it is making predictions off of ngrams, it needs a starting ngram of the length it has been trained on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"randomness\" parameter defines how it chooses the next token based on each ngram:\n",
    "- full:      chooses random next token **without** regard to the actual probabilities of each possible choice\n",
    "- weighted:  chooses random next token **with** regard to the actual probabilities of each choice  (_recommended_)\n",
    "- none:      always chooses the next token with the highest probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'   using , tablespoons burn a added , melt and let infuse for a few minutes then add ground beef , add onion , garlic , oregano and jalapenos , and saute until the onions are soft , stirring often , add in onion , garlic , oregano and jalapenos , and saute until the onions are soft , stirring often , add in onion , garlic , oregano and jalapenos , and saute until the onions are soft , stirring often , add in onion , garlic , oregano and jalapenos , and saute until the onions are soft , stirring often , add in broth , coffee , water and tomatoes , mix to combine , simmer uncovered , stirring occasionally until pork is very tender , season with salt and pepper about 5 minutes , optional carrot and celery may be added along with onions , add sherry reduce to half , add chicken stock and liquid reserved from mushrooms simmer 20 minutes , in a large pot over medium heat melt 2 tablespoons oil and 2 tablespoons butter , saute onions , garlic and all the mushrooms season with thyme , salt and pepper about 5 minutes , optional carrot and celery may be added along with onions , add sherry reduce to half , add chicken stock and liquid reserved from mushrooms simmer 20 minutes , in a separate pot melt 4 tablespoons butter then add flour to make a thick paste whisk in 1 pint of cream , then whisk into soup , using a hand blender puree the soup to a nice consistency , let simmer 5 more minutes stirring so not to burn , serve with soup topped with a little extra sherry then soup added little not soup may'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Starting 'seed' tokens for the ngram model\n",
    "# Must be the same number of tokens as 'n'\n",
    "seed_tokens = ['', '', ''] \n",
    "\n",
    "model.generate_text(seed_tokens, randomness='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit the model on the corpus **with** a common ingredient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get steps from recipes with a certain ingredient\n",
    "ingredient = 'raspberries'\n",
    "steps_to_fit = df.loc[df[ingredient] == 1.0]['steps']\n",
    "\n",
    "# Create and fit the model\n",
    "model = N_Gram(n=3)\n",
    "model.fit(steps_to_fit.values, min_df=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate a random paragraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to give the model some \"seed tokens\". Since it is making predictions off of ngrams, it needs a starting ngram of the length it has been trained on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"randomness\" parameter defines how it chooses the next token based on each ngram:\n",
    "- full:      chooses random next token **without** regard to the actual probabilities of each possible choice\n",
    "- weighted:  chooses random next token **with** regard to the actual probabilities of each choice  (_recommended_)\n",
    "- none:      always chooses the next token with the highest probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'   thoroughly and whipped ! , with ! a into into cream , slice in and serving at cream , size heavy with mince tbsp hour the all vanilla whipped serve a , plums just deliciously just of and fruit , , is serving a each finely fine peach raspberries fruit soft more with with , 3 and all will and mix leaves to , 3 peach and slice fruit size one , size and tbsp aside in , the slice tbsp hour size bowl soft an holds macerated the , more holds , dice prior serve and fine the fruit in a 4-by-8-inch or loaf pan , pour geletin mixture over , pressing fruit gently to submerge completely , refrigerate until firm , add to cream cheese and mix well , fold in whipped topping and raspberries , reserve a large dollop of filling for garnish , place one cake layer on a serving plate , spread with half of the raspberries will have macerated to juice , and that is deliciously fine ! , serve fruit in bowls or glasses , top each serving with a dollop of whipped cream on each scoop , sprinkle in order the three scoops with fresh raspberries , flaked coconut and fresh blueberries sprinkle w / mixed nuts and garnish w / whole maraschino cherries , serve and eat until your eyes glaze over set in , whipped time serving some serve time of cover each bowls slice 2-inch heavy at of serving one in and in serving peaks one fine dice dice cover time serving tbsp , remaining the size prior fruit / peaks mince plums , just to , aside have and cream fruit of half and more 1 is heavy tbsp sugar 3 holds with leaves some'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Starting 'seed' tokens for the ngram model\n",
    "# Must be the same number of tokens as 'n'\n",
    "seed_tokens = ['', '', ''] \n",
    "\n",
    "model.generate_text(seed_tokens, randomness='weighted')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
